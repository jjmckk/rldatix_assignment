{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff219d73-7939-406f-a57c-64f720bff811",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "> **Note:** Figures and detailed observations for each task are available in `task_1.ipynb` (classification) and `task_2.ipynb` (NLP entity extraction).\n",
    "\n",
    "### Approach\n",
    "\n",
    "The project had two main objectives:  \n",
    "1. Build a binary classification model to predict 30-day hospital readmission.  \n",
    "2. Use NLP to extract clinical entities from discharge notes using a large language model (LLM).\n",
    "\n",
    "**For the classification task:**\n",
    "- Cleaned and explored structured clinical data (e.g., age, number of previous admissions, length of stay).\n",
    "- Engineered features based on distributional differences and handled class imbalance using stratified sampling.\n",
    "- Trained a baseline random forest classifier and evaluated using ROC AUC, F1-score, and confusion matrix.\n",
    "- Used SHAP values to interpret feature influence.\n",
    "- Compared benchmark performance with a model trained on engineered features only.\n",
    "\n",
    "**For the NLP task:**\n",
    "- Used Flan-T5 in a zero-shot setting with custom prompts to extract clinical entities: diagnosis, treatment, symptoms, medication, and follow-up.\n",
    "- Compared predictions to manually annotated labels.\n",
    "- Observed risks common to LLMs in clinical contexts, including hallucination, entity ambiguity, and inconsistency across prompts.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Results\n",
    "\n",
    "#### Data Inspection\n",
    "\n",
    "- The dataset contained 200 rows and 9 columns, including structured fields and unstructured discharge notes.\n",
    "- Variables included both categorical and discrete numeric types such as `age`, `length_of_stay`, and `num_previous_admissions`.\n",
    "- All records were unique, with no missing values or duplicates.\n",
    "\n",
    "#### Feature Distribution\n",
    "\n",
    "- The outcome variable `readmitted_30_days` showed a moderate class imbalance (~1:2 ratio).\n",
    "- Most features had low cardinality; `age` had the most unique values (67), while others had 14 or fewer.\n",
    "- Discrete numeric features displayed varied distribution shapes, from right-skewed to multimodal.\n",
    "- Categorical features were generally well balanced.\n",
    "\n",
    "#### Distribution Across the Outcome Variable and Feature Engineering\n",
    "\n",
    "- No strong correlations were observed between features and the outcome.\n",
    "- `age` and `num_previous_admissions` showed mild correlation with `readmitted_30_days`.\n",
    "- Visual inspection suggested specific value ranges within features might be linked to outcome classes.\n",
    "- New features were derived based on these patterns.\n",
    "\n",
    "#### Classification Task\n",
    "\n",
    "- The random forest model performed poorly, achieving results only slightly better than random guessing and below baseline accuracy.\n",
    "- SHAP values identified `age`, `num_previous_admissions`, and `length_of_stay` as the most influential features, consistent with prior EDA.\n",
    "- The engineered-features-only model performed similarly, suggesting that most of the signal in the raw data was retained through feature transformation, but overall predictive power remained limited.\n",
    " \n",
    "#### LLM Entity Tagging Task\n",
    "\n",
    "- The Flan-T5 model achieved high recall across most entity types, particularly for `symptoms`, `medication`, and `follow-up`, with recall scores of 1.00 for each.\n",
    "- Precision was more variable: `medication` and `symptoms` achieved good precision (0.75 and 0.67 respectively), while `follow-up` (0.33) and `treatment` (0.57) showed more frequent false positives.\n",
    "- `diagnosis` was not detected in any sample, despite being present in two manually labelled cases. This resulted in a precision and recall of 0.00 for that entity.\n",
    "- Overall, the model demonstrated strong surface-level pattern matching, but limited ability to infer clinical meaning, especially for implied or resolved conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "#### Classification Model\n",
    "\n",
    "- The weak performance of the classification model suggests that the available structured data lacks strong predictive power for 30-day readmission. \n",
    "- Despite this, SHAP analysis highlighted a consistent set of influential features (`age`, `length_of_stay`, `num_previous_admissions`), which could inform future data collection or risk stratification approaches.\n",
    "- In a real-world setting, this model in its current form would not be reliable for deployment but could serve as a baseline for further refinement with additional features or richer data sources (e.g., lab results, clinical history).\n",
    "\n",
    "#### LLM Entity Tagging\n",
    "\n",
    "- The LLM was effective at identifying surface-level clinical information, with high recall for observable entities like `symptoms` and `medications`, which could support downstream tasks such as automated coding or triage.\n",
    "- However, its failure to detect `diagnosis` entities imits its current utility in decision-making contexts.\n",
    "- Precision variability across entity types also highlights the need for caution when integrating LLM outputs into clinical workflows, particularly where high accuracy and interpretability are critical.\n",
    "\n",
    "---\n",
    "\n",
    "### With More Time or Data\n",
    "\n",
    "#### Data\n",
    "- Expand the dataset to improve generalisability; 200 datapoints is relatively small for training and evaluation.\n",
    "- Add more detailed structured features, such as lab results, comorbidities, and prior treatments.\n",
    "- Incorporate embeddings from discharge notes into the classification model to capture unstructured clinical context.\n",
    "\n",
    "#### Modelling\n",
    "\n",
    "##### Classification Model\n",
    "- Experiment with alternative classifiers such as logistic regression or XGBoost.\n",
    "- Apply hyperparameter tuning to optimise model performance beyond the default random forest baseline.\n",
    "\n",
    "##### NLP Model\n",
    "- Fine-tune a domain-specific language model (e.g., ClinicalBERT) to improve recognition of subtle or implied entities.\n",
    "- Revise prompts with clear definitions to improve performance.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "##### Classification Model\n",
    "- Use cross-validation to make performance evaluation more robust and reduce reliance on a single train/test split.\n",
    "\n",
    "##### NLP Model\n",
    "- Increase the number of manually annotated discharge notes to improve the reliability of evaluation.\n",
    "- Define clear annotation guidelines to resolve ambiguity in entity definitions (e.g., resolved vs. active diagnoses)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
