{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c20700-ea9e-4978-bb5a-8340e554b3a4",
   "metadata": {},
   "source": [
    "# Data Science & LLM Technical Assessment Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee2e1e-c1ac-454d-87de-fea21b6ef08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = './data.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "#print dataframe shape\n",
    "print(\"Dataframe shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e794a-9f23-41ac-b5f6-0988ed0a7bdd",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09f327-7ce1-4bb7-875c-c657a7595ec6",
   "metadata": {},
   "source": [
    "### Sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd6855-2225-409a-be27-0c74e9a48807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create nlp dataframe\n",
    "nlp_df = df[['discharge_note','readmitted_30_days']].copy()\n",
    "\n",
    "#sample nlp_df\n",
    "nlp_df['discharge_note'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22e30f-fb59-4ad0-ac4b-0a8c5f4c1545",
   "metadata": {},
   "source": [
    "### Get descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6bfbf-b32f-45d2-8b1c-15585b3268eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "#download libs\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#create function to produce descriptive stats on text features\n",
    "def GetTextStats(text):\n",
    "    \"\"\"\n",
    "    :des:\n",
    "    :inp:\n",
    "    :out:\n",
    "    \"\"\"\n",
    "\n",
    "    #tokenize words in lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    #remove any non-alphabetic characters\n",
    "    words_clean = [w for w in words if w.isalpha()]\n",
    "\n",
    "    #create table with preditive stats\n",
    "    stats_df = pd.Series({\n",
    "        'char_count': len(text),\n",
    "        'word_count': len(words_clean),\n",
    "        'avg_word_length': sum(len(w) for w in words_clean) / max(len(words_clean), 1),\n",
    "        'unique_words': len(set(words_clean)),\n",
    "        'stopword_ratio': sum(w in stop_words for w in words_clean) / max(len(words_clean), 1),\n",
    "        'punctuation_count': sum(1 for c in text if c in string.punctuation)\n",
    "    })\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "#apply text_stats\n",
    "text_stats_df = nlp_df['discharge_note'].apply(GetTextStats)\n",
    "\n",
    "#combine with original df\n",
    "nlp_df = nlp_df.join(text_stats_df)\n",
    "\n",
    "#view df\n",
    "nlp_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974642f-5f3a-429c-ba57-e0f9b20b936b",
   "metadata": {},
   "source": [
    "### Visualise descriptive features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385b708-03fd-40ca-9ef1-9c9c78c27a94",
   "metadata": {},
   "source": [
    "#### char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a999a-3232-4acc-b6ea-343f5d1d8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='char_count',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Character Counts in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#update formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Character Count',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921eddf8-96ad-47f5-abae-1d11f208ecf1",
   "metadata": {},
   "source": [
    "#### word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3044ff-d842-4021-bd55-b007b05fe5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='word_count',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Word Counts in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#update formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Word Count',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18c48d-661e-4f05-bdd1-7e1d0ee61c26",
   "metadata": {},
   "source": [
    "#### avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b14f55-23aa-4f83-ac0f-33271c48e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='avg_word_length',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Average Word Length in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#update formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Average Word Length',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e6eee-4ed1-46a4-8ff4-da58685f9474",
   "metadata": {},
   "source": [
    "#### unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee7903-4fb1-41a3-ae20-29e8b0e5e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='unique_words',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Unique Words in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#update formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Unique Word Count',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d8f9a-8fc4-4b5c-9628-50182569b204",
   "metadata": {},
   "source": [
    "#### stopword_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276cab5-9d7e-4eb2-8893-0a8a78625ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='stopword_ratio',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Stopword Ratio in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#update formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Stopword Ratio',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bf001-84fb-4307-8f6c-46f4bc3f8f0a",
   "metadata": {},
   "source": [
    "#### punctuation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d79784-6db8-4745-a2cb-a62be8a099ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#create boxplot for character count\n",
    "fig = px.box(\n",
    "    nlp_df,\n",
    "    y='punctuation_count',\n",
    "    points='all',  # Show individual data points\n",
    "    title='Distribution of Puncutation Count in Discharge Notes',\n",
    "    boxmode='overlay'\n",
    ")\n",
    "\n",
    "#specify height and width\n",
    "height, width = 600, 600\n",
    "\n",
    "#formatting\n",
    "fig.update_layout(\n",
    "    yaxis_title='Punctuation Count',\n",
    "    template='plotly_white',\n",
    "    showlegend=False,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d57a44-c1ac-4954-a865-971f173f16e9",
   "metadata": {},
   "source": [
    "#### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c296e-a519-4a87-9774-7609f91f7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#assign stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#combine all discharge notes into one string\n",
    "text = \" \".join(nlp_df['discharge_note'].dropna().astype(str).tolist())\n",
    "\n",
    "#remove stopwords\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "#create word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    stopwords=stop_words,\n",
    "    collocations=False  # prevent joining of common word pairs\n",
    ").generate(filtered_text)\n",
    "\n",
    "#show plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243cea3-a8fe-4958-82c5-c4a0bfa43d7c",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b68274-e5a1-43cf-a37f-4d8e6440ae18",
   "metadata": {},
   "source": [
    "- Discharge notes were similar in form, showing small ranges in word count, number of unique words, and average word length when compared to the range typically observed in a broader corpus.  \n",
    "- The word cloud showed that neutral words such as *\"patient\"*, *\"advised\"*, and *\"discharged\"* were common. Words with negative medical sentiment (e.g., *\"complications\"*, *\"pneumonia\"*) also appeared, as well as words with positive sentiment such as *\"improvement\"*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97e6cb-b087-4921-affe-4da1795cf97c",
   "metadata": {},
   "source": [
    "### Wordcount between readmitted an non-readmitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b9d19-be09-4def-bc43-007acad4c57c",
   "metadata": {},
   "source": [
    "#### Split dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6adc53-6778-4a37-b829-0060e3aea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#ensure stopwords are available\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function to clean and tokenize discharge notes\n",
    "def preprocess_discharge_note(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "#apply tokenization before splitting\n",
    "nlp_df['clean_tokens'] = nlp_df['discharge_note'].apply(preprocess_discharge_note)\n",
    "\n",
    "#split by readmission status\n",
    "nlp_df_1 = nlp_df[nlp_df['readmitted_30_days'] == 1].copy()\n",
    "nlp_df_0 = nlp_df[nlp_df['readmitted_30_days'] == 0].copy()\n",
    "\n",
    "#show shape\n",
    "nlp_df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464e1f6-09da-4c99-b613-63709ae19140",
   "metadata": {},
   "source": [
    "#### Compare prevalent sentences across outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62831ea7-043e-4dd2-9041-c34b5cd0ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at number of unique values\n",
    "nlp_df['discharge_note'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9396d8-d7f0-480b-accc-07493dcbd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at value counts clean tokens\n",
    "nlp_df_1['clean_tokens'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5ee58-e185-4162-9b63-587595dd88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at value counts clean tokens\n",
    "nlp_df_0['clean_tokens'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32821d-a4fc-403b-83ad-c84198984692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get flat list of tokens\n",
    "tokens_0 = pd.Series([x for x in nlp_df_0['clean_tokens'].values for x in x])\n",
    "tokens_1 = pd.Series([x for x in nlp_df_1['clean_tokens'].values for x in x])\n",
    "\n",
    "#calculate value counts\n",
    "tokens_0_counts = tokens_0.value_counts()\n",
    "tokens_1_counts = tokens_1.value_counts()\n",
    "\n",
    "#combine into DataFrame\n",
    "token_counts_df = pd.DataFrame({\n",
    "    'non_readmitted_count': tokens_0_counts,\n",
    "    'readmitted_count': tokens_1_counts\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "#get total token counts per group\n",
    "total_tokens_0 = tokens_0.shape[0]\n",
    "total_tokens_1 = tokens_1.shape[0]\n",
    "\n",
    "#add proportion columns\n",
    "token_counts_df['prop_non_readmitted_count'] = token_counts_df['non_readmitted_count'] / total_tokens_0\n",
    "token_counts_df['prop_readmitted_count'] = token_counts_df['readmitted_count'] / total_tokens_1\n",
    "token_counts_df['prop_difference'] = abs(token_counts_df['prop_readmitted_count'] - token_counts_df['prop_non_readmitted_count'])\n",
    "\n",
    "#sort by by proportion difference\n",
    "token_counts_df.sort_values(by='prop_difference', ascending=False, inplace=True)\n",
    "\n",
    "#show df\n",
    "token_counts_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14c176-b459-4c03-b1c1-4254d29e3cb1",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8178e-b1d1-484c-b887-3e3cae36457b",
   "metadata": {},
   "source": [
    "- There are only 10 unique `discharge_notes` in the dataset.  \n",
    "- Proportional differences in specific terms can be observed across the outcome variable — for example, *\"blood pressure\"* appeared marginally more frequently in the positive class of the outcome variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465e696-3139-4f3b-b7de-7a3eaa5ead86",
   "metadata": {},
   "source": [
    "## Entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff27141-9d61-4e0a-8d8d-52b36fc8f2bd",
   "metadata": {},
   "source": [
    "### Create ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52209d02-599d-461e-841b-a35ae3f25686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#create unique discharge notes\n",
    "sentences = np.array([\n",
    "    'Good recovery trajectory. Follow-up scan scheduled next month.',\n",
    "    'Stable post-surgery. Advised to avoid physical exertion.',\n",
    "    'Symptoms controlled. Monitoring for relapse advised.',\n",
    "    'Discharge after recovery from pneumonia. No complications observed.',\n",
    "    'Patient discharged in stable condition. Recommend follow-up in 2 weeks.',\n",
    "    'Patient showed improvement. Prescribed antibiotics for 5 days.',\n",
    "    'Blood pressure under control. Continue current medication.',\n",
    "    'Patient discharged with minor discomfort. Advised rest and hydration.',\n",
    "    'No further signs of infection. Resume normal diet and activity.',\n",
    "    'Mild reaction to medication. Switched to alternative treatment.'\n",
    "])\n",
    "\n",
    "#create DataFrame\n",
    "ent_df = pd.DataFrame(sentences, columns=[\"discharge_note\"])\n",
    "\n",
    "#display the result\n",
    "ent_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08e8bb-0385-4d99-92b5-b2ea33bb86a5",
   "metadata": {},
   "source": [
    "### Flan-t5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47084bfc-f780-4ea3-ab8a-0463b099cc2c",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb347984-c775-43f5-b163-24e7deda17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#define model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "device = 'cpu'\n",
    "\n",
    "#load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd040b1-cdd3-4b3b-8a16-5033f0e9f721",
   "metadata": {},
   "source": [
    "#### Create function to extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c316e-87d7-442b-aaef-bb5b8bb5b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "#function to format a prompt using a discharge note\n",
    "def generate_prompt(note, custom_prompt=None):\n",
    "    assert custom_prompt is not None, \"You must provide a custom_prompt.\"\n",
    "    return custom_prompt.format(note=note)\n",
    "\n",
    "#function to run LLM inference\n",
    "def extract_entity(note, custom_prompt):\n",
    "    prompt = generate_prompt(note, custom_prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7732d-588a-46f6-8ff1-b3257033683b",
   "metadata": {},
   "source": [
    "#### Extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ddf40-4301-44e5-b0e8-08de1fe55937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define prompts\n",
    "diagnosis_prompt = \"\"\"Does the following discharge note contain a diagnosis? \n",
    "Answer with \"1\" if a diagnosis is present, or \"0\" if there is no diagnosis.\n",
    "Discharge note: \"{note}\"\n",
    "\"\"\"\n",
    "\n",
    "treatment_prompt = \"\"\"Does the following discharge note contain a treatment? \n",
    "Answer with \"1\" if a treatment is present, or \"0\" if there is no treatment.\n",
    "Discharge note: \"{note}\"\n",
    "\"\"\"\n",
    "\n",
    "symptoms_prompt = \"\"\"Does the following discharge note contain symptoms? \n",
    "Answer with \"1\" if symptoms are present, or \"0\" if there are no symptoms.\n",
    "Discharge note: \"{note}\"\n",
    "\"\"\"\n",
    "\n",
    "medications_prompt = \"\"\"Does the following discharge note mention any medications? \n",
    "Answer with \"1\" if a medication is present, or \"0\" if there is no medication.\n",
    "Discharge note: \"{note}\"\n",
    "\"\"\"\n",
    "\n",
    "followup_prompt = \"\"\"Does the following discharge note include any follow-up actions? \n",
    "Answer with \"1\" if a follow-up action is present, or \"0\" if there is no follow-up action.\n",
    "Discharge note: \"{note}\"\n",
    "\"\"\"\n",
    "\n",
    "#apply prompts\n",
    "ent_df['diagnosis'] = ent_df['discharge_note'].progress_apply(\n",
    "    lambda note: extract_entity(note, diagnosis_prompt)\n",
    ")\n",
    "\n",
    "ent_df['treatment'] = ent_df['discharge_note'].progress_apply(\n",
    "    lambda note: extract_entity(note, treatment_prompt)\n",
    ")\n",
    "\n",
    "ent_df['symptoms'] = ent_df['discharge_note'].progress_apply(\n",
    "    lambda note: extract_entity(note, symptoms_prompt)\n",
    ")\n",
    "\n",
    "ent_df['medications'] = ent_df['discharge_note'].progress_apply(\n",
    "    lambda note: extract_entity(note, medications_prompt)\n",
    ")\n",
    "\n",
    "ent_df['follow_up'] = ent_df['discharge_note'].progress_apply(\n",
    "    lambda note: extract_entity(note, followup_prompt)\n",
    ")\n",
    "\n",
    "#show df\n",
    "ent_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c14e5-fbb2-48a7-813a-651e747b689d",
   "metadata": {},
   "source": [
    "#### Check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de36d69-493f-46e4-bcf6-555c76755763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through text and show corresponding outputs\n",
    "for idx, row in ent_df.iterrows():\n",
    "    print(f\"[{idx}] {row['discharge_note']}\")\n",
    "    print(f\"     diagnosis={row['diagnosis']} | treatment={row['treatment']} | symptoms={row['symptoms']} | medications={row['medications']} | follow-up={row['follow_up']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b2fee-dc4f-44e7-ae18-ec41a806b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define true labels\n",
    "y_true = {\n",
    "    \"diagnosis\":      [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "    \"treatment\":      [0, 1, 0, 0, 0, 1, 1, 1, 0, 1],\n",
    "    \"symptoms\":       [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
    "    \"medication\":     [0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "    \"follow-up\":      [1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "#define predictions\n",
    "y_pred = {\n",
    "    \"diagnosis\":      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    \"treatment\":      [1, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
    "    \"symptoms\":       [0, 1, 1, 0, 0, 1, 1, 1, 0, 1],\n",
    "    \"medication\":     [0, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "    \"follow-up\":      [1, 0, 1, 0, 1, 1, 1, 1, 0, 0]\n",
    "}\n",
    "\n",
    "#create DataFrame with snake_case headers\n",
    "results_df = pd.DataFrame({\n",
    "    \"text_index\": list(range(10)),\n",
    "    \"diagnosis_true\": y_true[\"diagnosis\"],\n",
    "    \"diagnosis_pred\": y_pred[\"diagnosis\"],\n",
    "    \"treatment_true\": y_true[\"treatment\"],\n",
    "    \"treatment_pred\": y_pred[\"treatment\"],\n",
    "    \"symptoms_true\": y_true[\"symptoms\"],\n",
    "    \"symptoms_pred\": y_pred[\"symptoms\"],\n",
    "    \"medication_true\": y_true[\"medication\"],\n",
    "    \"medication_pred\": y_pred[\"medication\"],\n",
    "    \"follow_up_true\": y_true[\"follow-up\"],\n",
    "    \"follow_up_pred\": y_pred[\"follow-up\"],\n",
    "})\n",
    "\n",
    "#show DataFrame\n",
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a0eff-24c0-4d10-8e62-4c06f069420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "#calculate metrics for each entity\n",
    "metrics = []\n",
    "for entity in y_true.keys():\n",
    "    acc = accuracy_score(y_true[entity], y_pred[entity])\n",
    "    prec = precision_score(y_true[entity], y_pred[entity], zero_division=0)\n",
    "    rec = recall_score(y_true[entity], y_pred[entity], zero_division=0)\n",
    "    metrics.append({\n",
    "        \"entity\": entity,\n",
    "        \"accuracy\": round(acc, 2),\n",
    "        \"precision\": round(prec, 2),\n",
    "        \"recall\": round(rec, 2)\n",
    "    })\n",
    "\n",
    "#show df\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbc9be-7081-4835-9338-5e7f64238971",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c03b45-e2b9-4231-8d57-34811a67a45e",
   "metadata": {},
   "source": [
    "- Diagnosis: The model achieved high accuracy (0.80) but failed to identify any true positives, resulting in 0.00 precision and recall. This confirms earlier observations that the model struggles to detect diagnoses, especially when they are implied or refer to resolved conditions.\n",
    "- Treatment: Performance was mixed, with moderate accuracy (0.60), good recall (0.80), but lower precision (0.57), indicating the model often predicts treatment where it isn’t present (i.e. false positives).\n",
    "- Symptoms: Strong overall performance with high precision (0.67) and perfect recall (1.00). The model effectively identified all true symptom cases but included a few extra predictions.\n",
    "- Medication: The model performed best here, with high accuracy (0.90) and both precision and recall at or near 1.00, showing it reliably detects explicit medication mentions.\n",
    "- Follow-up: The model captured all true follow-up instances (recall = 1.00) but with lower precision (0.33), suggesting frequent over-prediction of follow-up when it was not actually mentioned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d5970-0566-47b5-b8e4-9e0b4dcf9d09",
   "metadata": {},
   "source": [
    "## Discussion on using LLMs for medical entity tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2643c89-f869-447f-bada-9f0eb24f5188",
   "metadata": {},
   "source": [
    "#### Risks and Limitations of Using LLMs in Clinical Entity Extraction\n",
    "\n",
    "Large language models (LLMs) like Flan-T5 offer strong zero-shot and few-shot capabilities, but applying them in clinical NLP tasks presents several challenges. These include hallucination, entity ambiguity, and limitations stemming from general-purpose training. My use of Flan-T5 to extract clinical entities from discharge notes highlights several of these risks:\n",
    "\n",
    "- Hallucination and Omission:  \n",
    "  When a model generates information that isn’t present in the input (hallucination) or fails to extract relevant information that is present (omission). In clinical tasks, this can lead to missing important diagnoses or fabricating details.\n",
    "\n",
    "- Entity Ambiguity:  \n",
    "  Occurs when similar or overlapping concepts (e.g., treatment vs medication) confuse the model, leading to inconsistent or incorrect tagging of entities.\n",
    "\n",
    "- Prompt Sensitivity and Inconsistency:  \n",
    "  Refers to how small changes in prompt wording or structure can lead to different outputs. Multi-entity prompts can overwhelm the model, reducing consistency across tasks.\n",
    "\n",
    "- Limitations of General-Purpose Models:  \n",
    "  General LLMs are not trained on domain-specific data like clinical notes, so they may lack medical knowledge, context awareness, or the ability to interpret subtle clinical language without fine-tuning.\n",
    "\n",
    "#### Observations from Model Output\n",
    "\n",
    "The Flan-T5 model showed all of these problems in my results. It missed both true diagnosis cases, even though the conditions (like pneumonia) were clearly mentioned. It also confused `treatment` and `medication`, especially in sample [9], where both were present but not tagged. When I used one prompt to extract all entities, the outputs were inconsistent — sometimes missing tags completely. This shows that while the model is capable, it’s not yet reliable for clinical tasks without more targeted training or support.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
